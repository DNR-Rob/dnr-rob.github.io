<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>RSS 2021 Workshop on Declarative and Neurosymbolic Representations in Robot Learning and Control</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo.svg" alt="" /></span> -->
						<h1>RSS 2021 Workshop on <br>Declarative and Neurosymbolic Representations in Robot Learning and Control</h1>
						<p>A Virtual Event with the <a href="https://roboticsconference.org/">2021 Robotics: Science and Systems (RSS) Conference</a><br />
						July 15th, 2021</p>
						<center>
							<img src="files/banner.svg" style="width: 75%;">
						</center>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">About</a></li>
							<li><a href="#first">Invited Speakers</a></li>
							<li><a href="#second">Accepted Papers</a></li>
							<li><a href="#schedule">Schedule</a></li>
							<li><a href="#organize">Organizing</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>About</h2>
										</header>
										<p>This is <b>a joint workshop with two themes</b> that share common interests and motivations.</p>
											<h3>Theme 1 - <b>Declarative knowledge in learning and control of robot behaviors:</b></h3>
											<p>For the purposes of explainability, abstraction, efficiency, or robustness, declarative knowledge is being incorporated into the decision-making process. We aim to explore novel ways to leverage complementary features of the different forms of decision making in order to inform the future research of deliberative systems relying on declarative knowledge that can be learned from, and shared with, humans.</p>

											<p><b>Knowledge representation and reasoning</b> is one of the earliest
											research topics in AI. From Prolog to PDDL, many declarative paradigms
											have been used in robot systems for representing human knowledge.
											However, people frequently find those methods not performing well in
											scalability and robustness, which is particularly important in
											robotics. <b>Numerical approaches to both planning and learning</b> have
											dominated recent developments in every level of robot behaviors.
											Current trends in explainable AI and neuro-symbolic reasoning attempt
											to marry some of the desirable features of traditional logic-based
											approaches with the generalization offered by more recent numerical
											methods.</p>

											<p>Under this theme, we intend to bring together robotics researchers
											from the once distant fields of knowledge representation and
											reasoning, symbolic and motion planning, reinforcement learning, and
											more generally machine learning for behavior recognition and
											synthesis, whose worlds are inevitably colliding. We aim to address
											the question of how to incorporate human knowledge in declarative
											forms into robot behaviors.</p>
											<h3>Theme 2 - <b>Neurosymbolic robotics for learning symbolic representations from sub-symbolic representations:</b></h3>
											<p>Neurosymbolic AI has emerged to integrate successful ideas in deep learning and classical symbolic reasoning in a single framework. Such a framework will have the desirable perception abilities of deep networks for bottom-up computation whilst allowing the system to make symbolic reasoning for top-down computation.</p>

											<p>Neurosymbolic approahces are underexplored in robotics and we believe that the advances in this field might produce a step-change towards truly intelligent robots as robots operate through low-level sensorimotor signals with high bandwidth yet reasoning requires high-level signals with low bandwidth. Essentially, "neurosymbolic robotics" can be a unified framework for high-level intelligence. Under this theme, we aim to draw attention to neurosymbolic AI methods, discuss their applicability and promising research directions in robotics. We will encourage participants to debate whether such an approach is feasible for robotics.</p>

											<p>We want to draw researchers that are interested in but not limited to deep learning, classical symbolic AI, logical reasoning, and knowledge representation. In particular: robotics researchers who work on symbolic AI approaches and would like to incorporate recent advances in deep learning, robotics researchers who work on deep learning and would like to exploit the reasoning and explainability related capabilities of symbolic manipulation systems. We also want to draw researchers from other AI fields (such as vision and natural language processing) who work in the intersection of neural and symbolic approaches, and cognitive and developmental roboticists who work on symbol grounding and emphasize symbol emergence.</p>
											

										<h3><b>Important dates:</b></h3>

										<ul style="list-style-type:none;">
											
											<li>Paper submission: <strike>6/20/2021</strike> 6/27/2021</li>
											<li>Acceptance notification: 6/30/2021</li>
											<li>Workshop: 7/15/2021</li>
										</ul>

										<h3><b>Submissions: </b></h3>
										<p>We accept <b>regular papers</b> (up to 8 pages of unpublished work) and <b>extended abstracts</b> (up to 4 pages of novel work or from a recently published paper) in standard RSS format, excluding unlimited pages for references. The review process will be single-blind. 10 minutes will be allocated for each regular paper, and 2 minutes for each extended abstract. There will also be a <b>poster presentation session</b> through <a href="https://gather.town/">gather.town</a> for discussions. </p>
										
										<p>Papers will be submitted through EasyChair: <a href="https://easychair.org/cfp/DNR-ROB-2021">https://easychair.org/cfp/DNR-ROB-2021</a></p>

										<p>There is a one-page CFP in PDF format: <a href="files/CFP.pdf">Call for Papers</a></p>
									</div>
								</div>
							</section>

						<!-- Invited Speakers -->
							<section id="first" class="main special">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
										<div class="row uniform" align="center">


					 						<div class="2u 12u$(small)">
												<a href="https://researcher.watson.ibm.com/researcher/view.php?person=ibm-Masataro.Asai" target="_blank" class="image">
													<span class="image">
														<img src="images/speaker_asai.jpg" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Masataro Asai<br><font size="3">IBM Research</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="https://eps.leeds.ac.uk/computing/staff/76/professor-anthony-g-cohn-freng-ceng-citp" target="_blank" class="image">
													<span class="image">
														<img src="images/speaker_cohn.png" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Anthony Cohn<br><font size="3">U of Leeds</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="https://www.cs.cmu.edu/~katef/" target="_blank" class="image">
													<span class="image">
														<img src="https://www.ml.cmu.edu/images/Fragkiadaki_Katerina_222.jpg" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Katerina Fragkiadaki<br><font size="3">CMU</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="https://www.robots.ox.ac.uk/~nickh/" target="_blank" class="image">
													<span class="image">
														<img src="https://eng.ox.ac.uk/media/1180/nick-hawes-provided.jpg?center=0.35796766743648961,0.47572815533980584&mode=crop&width=250&height=250&rnd=132441407640000000" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Nick Hawes<br><font size="3">U of Oxford</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="https://www.csail.mit.edu/person/leslie-kaelbling" target="_blank" class="image">
													<span class="image">
														<img src="https://www.csail.mit.edu/sites/default/files/styles/headshot/public/images/migration/kaelbling.jpg" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Leslie P. Kaelbling<br><font size="3">MIT</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="http://cs.brown.edu/people/gdk/" target="_blank" class="image">
													<span class="image">
														<img src="images/speaker_gdk.jpg" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">George Konidaris<br><font size="3">Brown</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="https://eecs.engin.umich.edu/people/kuipers-benjamin/" target="_blank" class="image">
													<span class="image">
														<img src="https://eecs.engin.umich.edu/wp-content/uploads/portraits/kuipers.jpg" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Ben Kuipers<br><font size="3">UMich</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="http://www.inf.ufrgs.br/~lamb/" target="_blank" class="image">
													<span class="image">
														<img src="https://i1.rgstatic.net/ii/profile.image/505683772096512-1497575814099_Q128/Luis-Lamb.jpg" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Luís C. Lamb<br><font size="3">UFRGS</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="https://www.csee.umbc.edu/people/faculty/cynthia-matuszek/" target="_blank" class="image">
													<span class="image">
														<img src="https://isrc.umbc.edu/files/2015/09/cynthia-e1479872806467.png" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Cynthia Matuszek<br><font size="3">UMBC</font></h4>
												</a>
											</div>

					 						<div class="2u 12u$(small)">
												<a href="https://www.cs.toronto.edu/~sheila/" target="_blank" class="image">
													<span class="image">
														<img src="images/speaker_McIlraith.jpg" alt="" height="180" width="180">
													</span>
													<h4 style="text-align: center;">Sheila McIlraith<br><font size="3">U of Toronto</font></h4>
												</a>
											</div>
								</div>
							</section>

						<!-- Accepted Papers -->
							<section id="second" class="main">
								<header class="major">
									<h2>Accepted Papers</h2>
								</header>
								<p>
									<b style="color: #2E86C1;">Perceptual Reasoning and Interactive Learning for Planning Urban Driving Behaviors</b><br>
									Cheng Cui, Saeid Amiri, Xingyue Zhan and Shiqi Zhang
								</p>
								<p>
									<b style="color: #2E86C1;">Composable Causality in Semantic Robot Programming</b><br>
									Emily Sheetz, Xiaotong Chen, Zhen Zeng, Kaizhi Zheng, Qiuyu Shi and Chad Jenkins
								</p>
								<p>
									<b style="color: #2E86C1;">Learning Symbolic Spatial Relationships with Object-centric Representation</b><br>
									Wentao Yuan, Chris Paxton, Karthik Desingh and Dieter Fox
								</p>
								<p>
									<b style="color: #2E86C1;">Planning from Pixels in Environments with Combinatorially Hard Search Spaces</b><br>
									Marco Bagatella, Mirek Olšák, Michal Rolínek and Georg Martius
								</p>
								<p>
									<b style="color: #2E86C1;">A Framework for Creative Problem Solving Through Action Discovery</b><br>
									Evana Gizzi, Mateo Guaman Castro, Wo Wei Lin and Jivko Sinapov
								</p>
								<p>
									<b style="color: #2E86C1;">Learning Goal-Based Abstractions from Human Teachers</b><br>
									Nakul Gopalan and Matthew Gombolay
								</p>
								<p>
									<b style="color: #2E86C1;">Learning Quadruped Locomotion Policies with Reward Machines</b><br>
									David DeFazio and Shiqi Zhang
								</p>
								<p>
									<b style="color: #2E86C1;">Efficient Hierarchical Navigation and Manipulation by Constraint-induced Option-Reward Design</b><br>
									Zhiao Huang, Xiaochen Li and Hao Su
								</p>
								<p>
									<b style="color: #2E86C1;">Learning Robot Manipulation Programs:A Neuro-symbolic Approach</b><br>
									Parag Singla, Rohan Paul, Rahul Jain and Vishwajeet Agrawal
								</p>
								<p>
									<b style="color: #2E86C1;">Towards Learning Grounding for Abstract Control Policies</b><br>
									Stevan Tomic, Federico Pecora and Alessandro Saffiotti
								</p>
								<p>
									<b style="color: #2E86C1;">Integrating Knowledge-based Reasoning and Data-driven Learning in Robotics</b><br>
									Mohan Sridharan
								</p>
							</section>

						<!-- Schedule -->
							<section id="schedule" class="main special" style="color: black;">
								<header class="major">
									<h2>Schedule</h2>
								</header>
								<div class="table-wrapper" style="text-align: left;">
									<table>
										<thead>
											<tr>
												<th style="width: 25%;">Time</th>
												<th style="width: 50%">Description</th>
											</tr>
										</thead>
										<tbody>
											<tr>
												<td>
													08:00 - 08:20 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													14:00 - 14:20 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													21:00 - 21:20 <a href="https://time.is/JST">(JST, July 15)</a>
												</td>
												<td>Introduction &amp; Opening Remarks</td>
											</tr>
											<tr style="background-color: #FFC2C7;">
												<td>
													08:20 - 08:40 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													14:20 - 14:40 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													21:20 - 21:40 <a href="https://time.is/JST">(JST, July 15)</a>
												</td>
												<td>Invited talk 1: <b>Benjamin Kuipers</b><br><br>
													Title: Acting, Learning, and Knowing in Large-Scale Space<br><br>
													Abstract: For an embodied mobile agent, human or robot, knowledge of its spatial environment (a “cognitive map”) can be learned from its changing perceptions, and can be used to plan how to act to achieve its goals.  The human cognitive map is remarkable for its ability to express and use multiple kinds of spatial knowledge:  of perceptual space, of small-scale and large-scale navigational space, and of metrical and topological relations within and among those spaces.  To have the flexibility and robustness of the human cognitive map, robots need the same.  Considering interactions between different kinds of spatial knowledge suggests roles for symbolic and neurosymbolic inference. <br><br>
													Bio: Benjamin Kuipers is a Professor of Computer Science and Engineering at the University of Michigan.  He was previously at the University of Texas at Austin, where he held an endowed professorship and served as Computer Science department chair.  He received his B.A. from Swarthmore College, his Ph.D. from MIT, and he is a Fellow of AAAI, IEEE, and AAAS.  His research in artificial intelligence and robotics has focused on the representation, learning, and use of foundational domains of commonsense knowledge, including knowledge of space, dynamical change, objects, and actions.  He is currently investigating ethics as a foundational domain of knowledge for robots and other AIs that may act as members of human society.
												</td>
											</tr>
											<tr style="background-color: #FFC2C7;">
												<td>
													08:40 - 09:00 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													14:40 - 15:00 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													21:40 - 22:00 <a href="https://time.is/JST">(JST, July 15)</a>
												</td>
												<td>Invited talk 2: Katerina Fragkiadaki</td>
											</tr>
											<tr style="background-color: #FFC2C7;">
												<td>
													09:00 - 09:20 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													15:00 - 15:20 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													22:00 - 22:20 <a href="https://time.is/JST">(JST, July 15)</a>
												</td>
												<td>Invited talk 3: Cynthia Matuszek</td>
											</tr>
											<tr style="background-color: #FFC2C7;">
												<td>
													09:20 - 10:00 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													15:20 - 16:00 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													22:20 - 23:00 <a href="https://time.is/JST">(JST, July 15)</a>
												</td>
												<td>
													Panel discussion 1<br>
													Chair: Roderic Grupen<br>
													Co-chair: Justus Piater
												</td>
											</tr>
											<tr>
												<td colspan="2" style="text-align: center;">
													RSS CONFERENCE BREAK
												</td>
											</tr>
											<tr style="background-color: #B6E5D8;">
												<td>
													13:00 - 13:20 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													19:00 - 19:20 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													02:00 - 02:20 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Invited talk 4: <b>Anthony Cohn</b><br><br>
													Title: Human-like planning for reaching in cluttered environments<br><br>
													Abstract: Humans, in comparison to robots, are remarkably adept at reaching for objects in cluttered environments. The best existing robot planners are based on random sampling of configuration space, which becomes excessively high dimensional with large number of objects. Consequently, most planners often fail to efficiently find object manipulation plans in such environments. We addressed this problem by identifying high-level manipulation plans in humans, and transferring these skills to robot planners. We used virtual reality to capture human participants reaching for a target object on a tabletop cluttered with obstacles. From this, we devised a qualitative representation of the task space to abstract the decision making, irrespective of the number of obstacles. Based on this representation, human demonstrations were segmented and used to train decision classifiers. Using these classifiers, our planner produced a list of waypoints in task space. These waypoints provided a high-level plan, which could be transferred to an arbitrary robot model and used to initialise a local trajectory optimiser. We evaluated this approach through testing on unseen human VR data, a physics-based robot simulation, and a real robot (dataset and code are publicly available). We found that the human-like planner outperformed a state-of-the art standard trajectory optimisation algorithm, and was able to generate effective strategies for rapid planning- irrespective of the number of obstacles in the environment. <br><br>
													Bio: Anthony  (Tony) Cohn is Professor of Automated Reasoning in the School of Computing, at the University of Leeds. His current research interests range from theoretical work on spatial calculi (receiving a KR test-of-time classic paper award in 2020) and spatial ontologies, to cognitive vision,  grounding language to vision, robotics, modelling spatial information in the hippocampus, and Decision Support Systems, particularly for the built environment. He is Editor-in-Chief Spatial Cognition and Computation and was previously Editor-in-chief of the AI journal. He is the recipient of the 2015 IJCAI Donald E Walker Distinguished Service Award, as well as the 2012 AAAI Distinguished Service Award. He is a Fellow of the Royal Academy of Engineering, the Alan Turing Institute in the UK, and is also a Fellow of AAAI, AISB, and  EurAI. 
												</td>
											</tr>
											<tr style="background-color: #B6E5D8;">
												<td>
													13:20 - 13:40 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													19:20 - 19:40 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													02:20 - 02:40 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Invited talk 5: <b>Nick Hawes</b><br><br>
													Title: Mission Planning with Uncertain Models<br><br>
													Abstract: Mission planning for long-horizon tasks requires the planning agent to use a model to encode its interaction with its environment. In most robotic tasks some parts of this model are known with certainty, whereas other parts may only be known with uncertainty at design time, and must be updated via learning either between missions (i.e. “offline") or during execution (“online"). In this talk I’ll give a high-level summary of our recent work on planning under uncertainty with such uncertain models. This will range from planning in MDPs with a Gaussian Process prior over a single state features,  to planning in Uncertain MDPs and Bayes-Adaptive MDPs where the true model cannot be known with certainty.<br><br>
													Bio: Nick Hawes is an Associate Professor in the Oxford Robotics Institute, part of the Department of Engineering Science at the University of Oxford, and a Fellow of Pembroke College. He leads the GOALS research group which performs research on problems in mission planning and decision making for autonomous system, particularly goal-oriented, long-lived robots acting in uncertain environments. He is an Associate Editor for the Journal of AI Research, and a Group Leader for AI and Robotics  at the UK's Turing Institute.
												</td>
											</tr>
											<tr style="background-color: #B6E5D8;">
												<td>
													13:40 - 14:00 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													19:40 - 20:00 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													02:40 - 03:00 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Invited talk 6: <b>George Konidaris</b><br><br>
													Title: Signal to Symbol (via Skills)<br><br>

													Abstract: I will address the question of how a robot should learn an
													abstract, task-specific representation of an environment. I will present
													a constructivist approach, where the computation the representation is
													required to support - here, planning using a given set of motor skills -
													is precisely defined, and then its properties are used to build the
													representation so that it is capable of doing so by construction. The
													result is a formal link between the skills available to a robot and the
													symbols it should use to plan with them. I will present an example of a
													robot autonomously learning a (sound and complete) abstract
													representation directly from sensorimotor data, and then using it to
													plan. I will also discuss ongoing work on making the resulting
													abstractions portable across tasks.<br><br>

													Bio: George Konidaris is the John E. Savage Assistant Professor of
													Computer Science at Brown and the Chief Roboticist of Realtime Robotics,
													a startup commercializing his work on hardware-accelerated motion
													planning. He holds a BScHons from the University of the Witwatersrand,
													an MSc from the University of Edinburgh, and a PhD from the University
													of Massachusetts Amherst. Prior to joining Brown, he held a faculty
													position at Duke and was a postdoctoral researcher at MIT. George is the
													recent recipient of an NSF CAREER award, young faculty awards from DARPA
													and the AFOSR, and the IJCAI-JAIR Best Paper Prize.
												</td>
											</tr>
											<tr style="background-color: #B6E5D8;">
												<td>
													14:00 - 14:20 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													20:00 - 20:20 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													03:00 - 03:20 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Invited talk 7: <b>Sheila McIlraith</b><br><br>
													Title: Building Human-Taskable Robots that Learn<br><br>

													Abstract: to be added<br><br>

													Bio: Sheila McIlraith is a Professor in the Department of Computer Science at the University of Toronto, a Canada CIFAR AI Chair (Vector Institute), and a Research Lead at the Schwartz Reisman Institute for Technology and Society. McIlraith's research is in the area of AI sequential decision making broadly construed, with a focus on human-compatible AI. Her research straddles machine learning and knowledge representation and reasoning. McIlraith is a Fellow of the ACM and AAAI.
												</td>
											</tr>
											<tr style="background-color: #B6E5D8;">
												<td>
													14:20 - 15:00 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													20:20 - 21:00 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													03:20 - 04:00 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>
													Panel discussion 2<br>
													Chair: Matteo Leonetti<br>
													Co-chair: Yuqian Jiang
												</td>
											</tr>
											<tr>
												<td colspan="2" style="text-align: center;">
													BREAK
												</td>
											</tr>
											<tr style="background-color: #FBE5C8;">
												<td>
													15:15 - 15:30 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													21:15 - 21:30 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													04:15 - 04:30 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Contributed talks</td>
											</tr>
											<tr style="background-color: #8FDDE7;">
												<td>
													15:30 - 15:50 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													21:30 - 21:50 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													04:30 - 04:50 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Invited talk 8: Masataro Asai</td>
											</tr>
											<tr style="background-color: #8FDDE7;">
												<td>
													15:50 - 16:10 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													21:50 - 22:10 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													04:50 - 05:10 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Invited talk 9: <b>Leslie P. Kaelbling</b><br><br>
													Title: Rich Representations for Rational Robots<br><br>
													Abstract: For robots to operate flexibly and intelligently in complex domains over long horizons, they will need to represent and ``reason'' with information about objects, space, physics, geometry, and people. They will need to represent their own uncertainty as well as possibly the beliefs and objectives of others, and the ways in which their own sensors and effectors connect to external reality. There is unlikely to be one representation that will serve all these purposes. I'll share some high-level ideas about, and some concrete technical progress toward, an approach that builds and uses multiple representations, creating them dynamically to address important subproblems as they arise. 
													<br><br>
													Bio: Leslie is a Professor in EECS at MIT. She has an undergraduate degree in Philosophy and a PhD in Computer Science from Stanford, and was previously on the faculty at Brown University. She was the founder of the Journal of Machine Learning Research. Her goal is to make robots that are as smart as you are.
												</td>
											</tr>
											<tr style="background-color: #8FDDE7;">
												<td>
													16:10 - 16:30 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													22:10 - 22:30 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													05:10 - 05:30 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Invited talk 10: Luís C. Lamb</td>
											</tr>
											<tr style="background-color: #8FDDE7;">
												<td>
													16:30 - 17:10 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													22:30 - 23:10 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													05:30 - 06:10 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>
													Panel discussion 3<br>
													Chair: Shiqi Zhang<br>
													Co-chair: Emre Ugur
												</td>
											</tr>
											<tr style="background-color: #FBE5C8;">
												<td>
													17:10 - 18:30 <a href="https://time.is/EDT">(EDT, July 15)</a><br>
													23:10 - 00:30 <a href="https://time.is/CEST">(CEST, July 15)</a><br>
													06:10 - 07:30 <a href="https://time.is/JST">(JST, July 16)</a>
												</td>
												<td>Poster session on Gather town</td>
											</tr>
										</tbody>
									</table>
								</div>
							</section>

						<!-- Organizing Committee -->
							<section id="organize" class="main special">
								<header class="major">
									<h2>Organizing Team</h2>
								</header>
									<ul align=left>
										<li><a href="https://alpera.xyz">Alper Ahmetoglu</a> (main contact), Bogazici University (Turkey), Email: ahmetoglu.alper@gmail.com</li>
										<li><a href="http://www.cs.binghamton.edu/~szhang/">Shiqi Zhang</a> (main contact), SUNY Binghamton (US), Email: zhangs@binghamton.edu</li>
										<br>
										<li><a href="http://www-robotics.cs.umass.edu/~grupen">Roderic Grupen</a>, University of Massachusetts (US)</li>
										<li><a href="https://yuqianjiang.us">Yuqian Jiang</a>, UT Austin (US)</li>
										<li><a href="https://eps.leeds.ac.uk/computing/staff/771/dr-matteo-leonetti">Matteo Leonetti</a>, University of Leeds (UK)</li>
										<li><a href="https://scholar.google.com/citations?user=QACV0cMAAAAJ">Tiffany Liu</a>, University of Massachusetts (US)</li>
										<li><a href="https://robotics.ozyegin.edu.tr/members/erhan-oztop/">Erhan Oztop</a>, Osaka University (Turkey), Ozyegin University (Japan) </li>
										<li><a href="https://iis.uibk.ac.at/people/justus">Justus Piater</a>, University of Innsbruck (Austria)</li>
										<li><a href="https://www.benjaminrosman.com">Benjamin Rosman</a>, University of Witwatersrand (South Africa)</li>
										<li><a href="http://www.tanichu.com">Tadahiro Taniguchi</a>, Ritsumeikan University (Japan)</li>
										<li><a href="https://www.cmpe.boun.edu.tr/~emre/">Emre Ugur</a>, Bogazici University (Turkey)</li>
									</ul>
							</section>
							<section id="acknowledgment" class="main">
								<header>
									<h3>Acknowledgments</h3>
								</header>
								This workshop is supported by TÜBİTAK (The Scientific and Technological Research Council of Turkey) ARDEB 1001 program (120E274).
							</section>
					</div>
				<p class="copyright">Design: <a href="https://html5up.net">HTML5 UP</a>.</p>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>